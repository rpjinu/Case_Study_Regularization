# -*- coding: utf-8 -*-
"""Case_Study_Regularization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wX1IYIw7KE_iotMAyL0mX0sSvZdLDfDv

## Problem Statement

There is a business requirement where we need to classify the gender based on the various features using the concept of regularization. Here we are going to apply the L1 and L2 regularization and using logistic regression we are going to apply the same. There are around 5000 records on which we need to create the model and come up with the solution.

**Data Dictionary**

**long_hair** - Length of hair

**forehead_width_cm** - Forehead width of individual

**forehead_height_cm** - Forehead height of individual

**nose_wide** -	Nose width of individual

**nose_long** -	Nose length of the individual

**lips_thin** -	lips structure of the individual

**distance_nose_to_lip_long** - Distance from nose to lip

**gender** -	Dependent variables i.e. Gender

# Table of Content

1. **[Import Libraries](#lib)**
2. **[Data Preparation](#prep)**
    - 2.1 - **[Understand the Data](#read)**
    - 2.2 - **[Exploratory Data Analysis](#eda)**
    - 2.3 - **[Missing Value Treatment](#null)**
    - 2.4 - **[Encoding and Feature Scaling](#enc)**
3. **[What is Regularization](#lr)**
    - 3.1 - **[Understanding the need of Regularization](#gi)**
    - 3.2 - **[Understanding the L2 Regularization or Ridge Regression](#gi)**
    - 3.3 - **[Understanding the L1 Regularization or Lasso Regression](#mf)**
    - 3.4 - **[Understanding the Elastic Net](#sf)**
4. **[Splitting the data into Train and Test](#sd)**
5. **[Creating the model on training dataset](#model)**
6. **[Run the model on the Test Dataset](#test)**
7. **[Check the accuracy of the model](#acc)**
    - 7.1 - **[Accuracy Score](#accscore)**
    - 7.2 - **[Confusion Matrix](#cm)**
    - 7.3 - **[ROC Curve](#roc)**
    - 7.4 - **[F1 Score](#f1score)**
    - 7.5 - **[Log Loss](#logloss)**
8. **[Comparing the Training and Testing Accuracies](#overunder)**
9. **[Applying K-Fold Cross Validation to find the best value of Lamda](#kfcv)**

<a id="lib"></a>
# 1. Import Libraries
"""

#Importing the libraries which will be helpful for the data analysis.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""<a id="prep"></a>
# 2. Data Preparation
"""

#Importing the dataset which we will use for the modelling
dataset = pd.read_csv('/content/gender_classification_v7.csv')

"""<a id="read"></a>
# 2.1. Understand the Data
"""

#Here are the few commands which will help us to understand the basic data
#The info command will help us to understand the different columns present in the dataset and its datatype
dataset.info()

#Len command will help us understand the total number of records present in the dataset
len(dataset)

#.columns command will help us understand the columns present in the dataset
dataset.columns

dataset

#The below command will help us understand the total number of columns present in the dataset
len(dataset.columns)

"""<a id="eda"></a>
# 2.2. Exploratory Data Analysis
"""

plt.figure(figsize=(15,2))
sns.pairplot(dataset[['forehead_width_cm','forehead_height_cm','gender']],hue='gender')
plt.show()

"""**From the above chart we can see that the forehead width and height is less for the females**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x='long_hair',hue='gender')

"""**From the above chart we can see that both males and females are having short as well as long hair**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x='nose_wide',hue='gender')

"""**From the above chart we can see that the nose width is more for males as compare to females**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x='nose_long',hue='gender')

"""**From the above chart we can see that the nose are longer for the males as compare to females**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x='lips_thin',hue='gender')

"""**From the above chart we can see that the lips are thinner for males than the females**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x='distance_nose_to_lip_long',hue='gender')

"""**From the above chart we can see that the distance from the nose to the lip is more for the males than the females**

<a id="null"></a>
# 2.3. Missing Value Treatment
"""

#Checking the count of the missing values percentage, there are very few missing values there in the dataset
dataset.isnull().sum()/len(dataset)*100

#Missing Value Imputation - We can impute the missing values using the methods of mean, median and mode based on the various scenarios
#When there is a numerical field we can populate the missing values using mean or median,
#if there are outliers in the dataset we use to populate the missing values using median else mean
#When we want to populate the missing values in the categorial files we go with mode as an option

"""<a id="enc"></a>
# 2.4. Encoding and Feature Scaling
"""

# Separating the numerical and categorical columns
from sklearn.preprocessing import StandardScaler
def data_type(dataset):
    """
    Function to identify the numerical and categorical data columns
    :param dataset: Dataframe
    :return: list of numerical and categorical columns
    """
    numerical = []
    categorical = []
    for i in dataset.columns:
        if dataset[i].dtype == 'int64' or dataset[i].dtype == 'float64':
            numerical.append(i)
        else:
            categorical.append(i)
    return numerical, categorical


numerical, categorical = data_type(dataset)

# Identifying the binary columns and ignoring them from scaling
def binary_columns(df):
    """
    Generates a list of binary columns in a dataframe.
    """
    binary_cols = []
    for col in df.select_dtypes(include=['int', 'float']).columns:
        unique_values = df[col].unique()
        if np.in1d(unique_values, [0, 1]).all():
            binary_cols.append(col)
    return binary_cols

binary_cols = binary_columns(dataset)

# Remove the binary columns from the numerical columns
numerical = [i for i in numerical if i not in binary_cols]

def encoding(dataset, categorical):
    """
    Function to automate the process of encoding the categorical data
    :param dataset: Dataframe
    :param categorical: List of categorical columns
    :return: Dataframe
    """
    for i in categorical:
        dataset[i] = dataset[i].astype('category')
        dataset[i] = dataset[i].cat.codes
    return dataset

dataset = encoding(dataset, categorical)

def feature_scaling(dataset, numerical):
    """
    Function to automate the process of feature scaling the numerical data
    :param dataset: Dataframe
    :param numerical: List of numerical columns
    :return: Dataframe
    """
    sc_x = StandardScaler()
    dataset[numerical] = sc_x.fit_transform(dataset[numerical])
    return dataset

dataset = feature_scaling(dataset, numerical)

dataset

"""<a id="gi"></a>
# 3.1. Understanding the need of Regularization

<a id="mf"></a>
# 3.2. Understanding the L2 Regularization or Ridge Regression
"""

#Splitting all the independent variables in one array
x = dataset.iloc[:,0:7].values

#Splitting the dependent variable in one array
y = dataset.iloc[:,-1].values

#Splitting the dataset into train and test based on the 70-30 ratio
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30)

"""<a id="model"></a>
# 5. Creating the model on training dataset
"""

#Applying the Logistic Regression on the training dataset
from sklearn.linear_model import LogisticRegression
logmodel_ini = LogisticRegression()
logmodel_ini.fit(x_train,y_train)

"""<a id="test"></a>
# 6. Run the model on the Test Dataset
"""

#Running the model on the test dataset
y_pred_ini = logmodel_ini.predict(x_test)

"""<a id="acc"></a>
# 7. Check the accuracy of the model

There are various ways to check the accuracy of the classification model we are going to use all the ways to check the accuracies
"""

#Importing all the functions to for checking the accuracies
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, log_loss,roc_curve, auc,RocCurveDisplay, PrecisionRecallDisplay,ConfusionMatrixDisplay

"""<a id="accscore"></a>
# 7.1. Accuracy Score
"""

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_test,y_pred_ini)

"""<a id="cm"></a>
# 7.2. Confusion Matrix
"""

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred_ini, labels=logmodel_ini.classes_)

# Create the ConfusionMatrixDisplay instance
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logmodel_ini.classes_)

# Plot the confusion matrix
disp.plot()
plt.show()

"""<a id="roc"></a>
# 7.3. ROC Curve
"""

# Assuming y_true are the true binary labels
# and y_scores are the probability estimates of the positive class
fpr, tpr, thresholds = roc_curve(y_test, y_pred_ini)
roc_auc = auc(fpr, tpr)

# Create RocCurveDisplay
display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='LogisticRegression')

# Plot the ROC curve
display.plot()
plt.title("Plot of ROC Curve for LR Model")
plt.show()

"""<a id="f1score"></a>
# 7.4. F1 Score
"""

#Using F1 Score we are checking the accuracy on the testing dataset
target_names= ["Negative(0)","Positive(1)"]
# Classification Report
print(classification_report(y_test,logmodel_ini.predict(x_test),target_names=target_names))

"""<a id="logloss"></a>
# 7.5. Log Loss
"""

#Using Logloss we are checking the accuracy on the testing dataset
log_loss(y_test,logmodel_ini.predict(x_test))

"""<a id="overunder"></a>
# 8. Comparing the Training and Testing Accuracies
"""

#Storing the predicted values of training dataset in y_pred_train
y_pred_train = logmodel_ini.predict(x_train)

#Checking the accuracy of training dataset
accuracy_score(y_train,y_pred_train)

#Checking the accuracy of testing dataset
accuracy_score(y_test,y_pred_ini)

"""**Conclusion:** As there is very less difference between the accuracy of training and testing dataset we are good to go with the model

<a id="kfcv"></a>
# 9. Applying K-Fold Cross Validation to find the best value of C (C = 1/Lambda)
"""

#Using K-fold cross validation technique we will find the best value of C
C = [0.01,0.1,1,10,100,1000]
from sklearn.model_selection import cross_val_score
cv_score = []

for c in C:
    logmodel = LogisticRegression(C=c)
    scores = cross_val_score(logmodel, x_train,y_train,cv=3, scoring='accuracy')
    cv_score.append(scores.mean())

cv_score
plt.plot(C, cv_score)
plt.show()
#The best value of depth is coming out to be 7, we will retrain the model with the value of depth as 7

#Applying the Logistic Regression on the training dataset
from sklearn.linear_model import LogisticRegression
logmodel_ridge = LogisticRegression(C=0.01,penalty='l2',solver='liblinear')
logmodel_ridge.fit(x_train,y_train)

logmodel_ridge.coef_

#Running the model on the test dataset
y_pred_ridge = logmodel_ridge.predict(x_test)

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_test,y_pred_ridge)

#Running the model on the test dataset
y_pred_train_ridge = logmodel_ridge.predict(x_train)

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_train,y_pred_train_ridge)

#Applying the Logistic Regression on the training dataset
from sklearn.linear_model import LogisticRegression
logmodel_lasso = LogisticRegression(C=0.01,penalty='l1',solver='liblinear')
logmodel_lasso.fit(x_train,y_train)

logmodel_lasso.coef_

#Running the model on the test dataset
y_pred_lasso = logmodel_lasso.predict(x_test)

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_test,y_pred_lasso)

#Running the model on the train dataset
y_pred_train_lasso = logmodel_lasso.predict(x_train)

#Using accuracy score we are checking the accuracy on the training dataset
accuracy_score(y_train,y_pred_train_lasso)

"""# Comparison of Lasso and Ridge models"""

logmodel_lasso.coef_

logmodel_ridge.coef_

"""**From the above coefficients we can say that the first feature i.e. long hair is not a very important variable that is the reason in the lasso logistic regression we can see the coefficient coming as 0, however, in the ridge we are seeing a lower coefficient. We also saw this in the exploratory data analysis that there are almost same number of males and females with long and short hairs.**"""

#Applying the Logistic Regression on the training dataset with Elastic net, we are adding l1_ratio = 0.5
#that means we are adding boht L1 and L2.
#The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'.
#Setting l1_ratio=0 is equivalent to using penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'.
#For 0 < l1_ratio <1, the penalty is a combination of L1 and L2.
from sklearn.linear_model import LogisticRegression
logmodel_elasticnet = LogisticRegression(C=0.01,penalty='elasticnet',solver='saga',l1_ratio=0.5)
logmodel_elasticnet.fit(x_train,y_train)

logmodel_elasticnet.coef_

#Running the model on the test dataset
y_pred_elasticnet = logmodel_elasticnet.predict(x_test)

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_test,y_pred_elasticnet)

#Running the model on the train dataset
y_pred_train_elasticnet = logmodel_elasticnet.predict(x_train)

#Using accuracy score we are checking the accuracy on the training dataset
accuracy_score(y_train,y_pred_train_elasticnet)

# create an empty dataframe to store the scores for various algorithms
from sklearn.metrics import roc_auc_score, precision_score, recall_score, roc_auc_score, f1_score
score_card = pd.DataFrame(columns=['model_name','Accuracy Score','Precision Score','Recall Score','AUC Score','f1 Score'])

# append the result table for all performance scores

def update_score_card(y_test,y_pred,model_name):

    # assign 'score_card' as global variable
    global score_card

    # append the results to the dataframe 'score_card'
    # 'ignore_index = True' do not consider the index labels
    score_card = pd.concat([score_card,pd.DataFrame([{'model_name':model_name,
                                    'Accuracy Score' : accuracy_score(y_test, y_pred),
                                    'Precision Score': precision_score(y_test, y_pred),
                                    'Recall Score': recall_score(y_test, y_pred),
                                    'AUC Score': roc_auc_score(y_test, y_pred),
                                    'f1 Score': f1_score(y_test, y_pred)}])],
                                    ignore_index = True)

update_score_card(y_test,y_pred_ini,'initial_model')

update_score_card(y_test,y_pred_ridge,'Ridge Regression - L2 Reg')

update_score_card(y_test,y_pred_lasso,'Lasso Regression - L1 Reg')

update_score_card(y_test,y_pred_elasticnet,'Elastic Net - L1 and L2')

score_card

"""**Interpretation: As per the above table, the initial model and the Elasticnet are giving the same accuracy score, however if the use case is to get the output faster i.e. low latency system we can go with L1 regularization**"""